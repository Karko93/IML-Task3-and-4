{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import Dataset\n",
    "import torch.utils.data as datatorch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import torch\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import torch.backends.cudnn as cudnn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_triplets = np.loadtxt('train_triplets.txt', dtype= 'str')\n",
    "test_triplets = np.loadtxt('test_triplets.txt', dtype= 'str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(59544, 3)\n"
     ]
    }
   ],
   "source": [
    "print(test_triplets.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(48206, 3)\n",
      "(43385, 3) (4821, 3)\n",
      "2410\n",
      "(4821,)\n"
     ]
    }
   ],
   "source": [
    "print(train_triplets.shape)\n",
    "#print()\n",
    "train_triplets , val_triplets = train_test_split(train_triplets, test_size = 0.1)\n",
    "print(train_triplets.shape, val_triplets.shape)\n",
    "half_index = np.int64((val_triplets.shape[0]-val_triplets.shape[0]%2)/2)\n",
    "print(half_index)\n",
    "val_labels = np.int64(np.ones((val_triplets.shape[0],)))\n",
    "print(val_labels.shape)\n",
    "val_triplets[half_index:, 0], val_triplets[half_index:, 1] = val_triplets[half_index:, 1], val_triplets[half_index:, 0].copy()\n",
    "val_labels[half_index:] = np.int64(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 ... 0 0 0]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = 'food/food'\n",
    "train_files = os.listdir(train_dir)\n",
    "test_files = os.listdir(train_dir)\n",
    "\n",
    "\n",
    "class ImageTriplesSet(Dataset):\n",
    "    def __init__(self , file_array, dir, mode='train', transform = None,labels =None):\n",
    "        self.triple_list = list(map(tuple, file_array))\n",
    "        self.mode = mode\n",
    "        self.labels = labels\n",
    "        self.dir = dir\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.triple_list)\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        img1 = Image.open(os.path.join(self.dir, self.triple_list[idx][0] + '.jpg'))\n",
    "        img2 = Image.open(os.path.join(self.dir, self.triple_list[idx][1] + '.jpg'))\n",
    "        img3 = Image.open(os.path.join(self.dir, self.triple_list[idx][2] + '.jpg'))\n",
    "        \n",
    "        \n",
    "        if self.transform is not None:\n",
    "            img1 = self.transform(img1).numpy()\n",
    "            img2 = self.transform(img2).numpy()\n",
    "            img3 = self.transform(img3).numpy()\n",
    "        if self.labels is None:\n",
    "            return img1, img2, img3\n",
    "        else:\n",
    "            return img1, img2, img3, self.labels[idx]\n",
    "            \n",
    "        #concat_img = cv2.hconcat([img1, img2, img3]).astype('float32')\n",
    "        #if self.mode == 'train':\n",
    "            #label = self.labels[idx]\n",
    "            #return concat_img , label\n",
    "            \n",
    "        #else:\n",
    "            #return concat_img, int(self.triple_list[idx][:-4])\n",
    "        \n",
    "#data_transform = transforms.Compose([\n",
    "  #  transforms.Resize(350,240),\n",
    "  #  transforms.CenterCrop(240),\n",
    "  #  transforms.ToTensor()\n",
    "#])\n",
    "\n",
    "data_transform = transforms.Compose([\n",
    "        transforms.Resize(224),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[\n",
    "                             0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "train_dataset = ImageTriplesSet(train_triplets, train_dir, transform = data_transform, labels = None)\n",
    "val_dataset = ImageTriplesSet(val_triplets, train_dir, transform= data_transform, labels = None)\n",
    "test_dataset = ImageTriplesSet(test_triplets, train_dir, mode=\"test\" ,transform = data_transform,labels = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\user/.cache\\torch\\hub\\pytorch_vision_master\n"
     ]
    }
   ],
   "source": [
    "model = torch.hub.load('pytorch/vision', 'resnet18', pretrained=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss:  1.040884252517454\n",
      "training loss:  0.979748300967678\n",
      "training loss:  0.915776168146441\n",
      "training loss:  0.9352488979216544\n",
      "training loss:  0.9094939789464397\n",
      "training loss:  0.9537016499427057\n",
      "training loss:  0.922075544634173\n",
      "training loss:  0.887079696501455\n",
      "training loss:  0.8771317716567747\n",
      "training loss:  0.9296222790595023\n",
      "training loss:  0.9185216599895109\n",
      "training loss:  0.87112937050481\n",
      "training loss:  0.9057048636098062\n",
      "training loss:  0.8523079406830573\n",
      "training loss:  0.9048811754872722\n",
      "training loss:  0.9071318814831395\n",
      "training loss:  0.8353641110081826\n",
      "training loss:  0.8442666742109484\n",
      "training loss:  0.8404120322196714\n",
      "training loss:  0.855715899698196\n",
      "training loss:  0.8876504705798242\n",
      "training loss:  0.8705182729228851\n",
      "training loss:  0.8439703891354222\n",
      "training loss:  0.8348568216446908\n",
      "training loss:  0.8278046384934457\n",
      "training loss:  0.8568702513171781\n",
      "training loss:  0.8890674402636867\n",
      "training loss:  0.8457466162020161\n",
      "training loss:  0.8050368332093761\n",
      "training loss:  0.8168074911640536\n",
      "training loss:  0.8412498581794\n",
      "training loss:  0.7563894458355442\n",
      "training loss:  0.8325761325897709\n",
      "training loss:  0.8155498389274843\n",
      "training loss:  0.8513966914146177\n",
      "training loss:  0.7863111784381251\n",
      "training loss:  0.7812901389214301\n",
      "training loss:  0.7659287375788535\n",
      "training loss:  0.8102987274046867\n",
      "training loss:  0.7994335351451751\n",
      "training loss:  0.7532161224272943\n",
      "training loss:  0.7886860466772511\n",
      "training loss:  0.7738686607730004\n",
      "training loss:  0.7573489700594256\n",
      "training loss:  0.782356729430537\n",
      "training loss:  0.7766877528159849\n",
      "training loss:  0.7538392870656906\n",
      "training loss:  0.7731844417510494\n",
      "training loss:  0.7033355139916943\n",
      "training loss:  0.7124999377035326\n",
      "training loss:  0.747982326053804\n",
      "training loss:  0.6201164876261065\n",
      "training loss:  0.7032314577410298\n",
      "training loss:  0.7411084213564473\n",
      "training loss:  0.7156401709202798\n",
      "training loss:  0.693214314599191\n",
      "training loss:  0.6675036222703995\n",
      "training loss:  0.6660944146494712\n",
      "training loss:  0.6501218618885163\n",
      "training loss:  0.6934812626531047\n",
      "training loss:  0.6231321634784821\n",
      "training loss:  0.6285698471530792\n",
      "training loss:  0.5871195697015331\n",
      "training loss:  0.5635944181872953\n",
      "training loss:  0.6383036778819177\n",
      "training loss:  0.6046124321799125\n",
      "training loss:  0.5614621129728132\n",
      "training loss:  0.5824984148625405\n",
      "training loss:  0.5726686552647622\n",
      "training loss:  0.5839970390642842\n",
      "training loss:  0.5728379170740804\n",
      "training loss:  0.5619601366981384\n",
      "training loss:  0.5392790302153556\n",
      "training loss:  0.541747581093542\n",
      "training loss:  0.5967100041527902\n",
      "training loss:  0.5830016914875277\n",
      "training loss:  0.564725891236336\n",
      "training loss:  0.5814334269492857\n",
      "training loss:  0.58059599611067\n",
      "training loss:  0.5697631720573672\n",
      "training loss:  0.5300542654529694\n",
      "training loss:  0.514613195773094\n",
      "training loss:  0.5324821539463536\n",
      "training loss:  0.4869783040015928\n",
      "training loss:  0.5564782542567099\n",
      "training loss:  0.5138714948008137\n",
      "training loss:  0.5570553262387553\n",
      "training loss:  0.48829103381403033\n",
      "training loss:  0.5389440271162218\n",
      "training loss:  0.4805424136500205\n",
      "training loss:  0.5279103248350082\n",
      "training loss:  0.5168714061860116\n",
      "training loss:  0.5532456636428833\n",
      "training loss:  0.5082324387565735\n",
      "training loss:  0.5195727694419122\n",
      "training loss:  0.4532851485475417\n",
      "training loss:  0.5287462626734087\n",
      "training loss:  0.5042164229577587\n",
      "training loss:  0.48500645352948096\n",
      "training loss:  0.5067700061105913\n",
      "training loss:  0.47258026369156375\n",
      "training loss:  0.5125859831610033\n",
      "training loss:  0.49841494617923615\n",
      "training loss:  0.42970974551093194\n",
      "training loss:  0.5155090949227733\n",
      "training loss:  0.46824312594629103\n",
      "training loss:  0.4676931529275833\n",
      "training loss:  0.5015640792346769\n",
      "training loss:  0.4595588263004057\n",
      "training loss:  0.5046608395153477\n",
      "training loss:  0.4579168586961685\n",
      "training loss:  0.45451636372074006\n",
      "training loss:  0.4740635493109303\n",
      "training loss:  0.46567395617884977\n",
      "training loss:  0.42828308093932366\n",
      "training loss:  0.42358782550980967\n",
      "training loss:  0.46555571786818967\n",
      "training loss:  0.43726753852059763\n",
      "training loss:  0.4672952765418637\n",
      "training loss:  0.4475631949401671\n",
      "training loss:  0.33868579951024824\n",
      "training loss:  0.3165509537343056\n",
      "training loss:  0.33584037519270377\n",
      "training loss:  0.3348156434874381\n",
      "training loss:  0.31987813932280384\n",
      "training loss:  0.3250054558438639\n",
      "training loss:  0.33587912782546014\n",
      "training loss:  0.3822046849996813\n",
      "training loss:  0.3678958872633596\n",
      "training loss:  0.33193067196876774\n",
      "training loss:  0.35638641349730954\n",
      "training loss:  0.35210368950520793\n",
      "training loss:  0.34679440961730096\n",
      "training loss:  0.35659082041632745\n",
      "training loss:  0.3194046914577484\n",
      "training loss:  0.346566631428657\n",
      "training loss:  0.3212143572107438\n",
      "training loss:  0.33226289528031505\n",
      "training loss:  0.3133448698828297\n",
      "training loss:  0.30682227351973135\n",
      "training loss:  0.28839808510195825\n",
      "training loss:  0.32798281696534926\n",
      "training loss:  0.30085573417525135\n",
      "training loss:  0.3721948761132456\n",
      "training loss:  0.32454015458783797\n",
      "training loss:  0.28813853283082286\n",
      "training loss:  0.3010817862326099\n",
      "training loss:  0.3301989907218564\n",
      "training loss:  0.32487074117506703\n",
      "training loss:  0.36542718256673506\n",
      "training loss:  0.28922514569374824\n",
      "training loss:  0.34406550853483137\n",
      "training loss:  0.32644542234559215\n",
      "training loss:  0.32704418657287476\n",
      "training loss:  0.3122488409280777\n",
      "training loss:  0.30893994963938193\n",
      "training loss:  0.3529958066440398\n",
      "training loss:  0.3199827190368406\n",
      "training loss:  0.3180892265612079\n",
      "training loss:  0.28955152726942496\n",
      "training loss:  0.31864227066116946\n",
      "training loss:  0.32785589560385675\n",
      "training loss:  0.2945363348530185\n",
      "training loss:  0.3199889929063858\n",
      "training loss:  0.30645776900552935\n",
      "training loss:  0.31034431390223965\n",
      "training loss:  0.30624664070144775\n",
      "training loss:  0.32120787136016354\n",
      "training loss:  0.290684629351862\n",
      "training loss:  0.30164233763371745\n",
      "training loss:  0.32099857109208263\n",
      "training loss:  0.29134018863401107\n",
      "training loss:  0.2967450013083796\n",
      "training loss:  0.27891981121032466\n",
      "training loss:  0.27872996176442794\n",
      "training loss:  0.30046106442328424\n",
      "training loss:  0.2924880798785917\n",
      "training loss:  0.2829013111129884\n",
      "training loss:  0.2858852263419859\n",
      "training loss:  0.31116626435710537\n",
      "training loss:  0.2066728607300789\n",
      "training loss:  0.21827698715271487\n",
      "training loss:  0.18345952562747464\n",
      "training loss:  0.2068398614083567\n",
      "training loss:  0.18260480751914362\n",
      "training loss:  0.18699865427709395\n",
      "training loss:  0.20239534926029942\n",
      "training loss:  0.2084248335130753\n",
      "training loss:  0.2025346626197138\n",
      "training loss:  0.19718493857691366\n",
      "training loss:  0.20942694141018775\n",
      "training loss:  0.2075224369764328\n",
      "training loss:  0.1970036260543331\n",
      "training loss:  0.2033329163828204\n",
      "training loss:  0.18996112817718136\n",
      "training loss:  0.19459779897043783\n",
      "training loss:  0.19513686866529525\n",
      "training loss:  0.20969597610735125\n",
      "training loss:  0.18144336198606797\n",
      "training loss:  0.21855611666556327\n",
      "training loss:  0.22831292546564533\n",
      "training loss:  0.21135220075807265\n",
      "training loss:  0.1801283133606757\n",
      "training loss:  0.23828009636171402\n",
      "training loss:  0.20997364722913311\n",
      "training loss:  0.21335939774590154\n",
      "training loss:  0.20467510723298596\n",
      "training loss:  0.19360416022039229\n",
      "training loss:  0.20202301490691402\n",
      "training loss:  0.24307197236245678\n",
      "training loss:  0.21845324914301595\n",
      "training loss:  0.19271732338013187\n",
      "training loss:  0.1909198410088016\n",
      "training loss:  0.2073551502919966\n",
      "training loss:  0.19759831601573574\n",
      "training loss:  0.19302760545284517\n",
      "training loss:  0.20318216758389626\n",
      "training loss:  0.20631086826324463\n",
      "training loss:  0.22097641614175612\n",
      "training loss:  0.1704324839576598\n",
      "training loss:  0.2027037033150273\n",
      "training loss:  0.2211399943597855\n",
      "training loss:  0.18874257466485422\n",
      "training loss:  0.19527012253961257\n",
      "training loss:  0.20260837193458311\n",
      "training loss:  0.18244857124743924\n",
      "training loss:  0.22432618900652854\n",
      "training loss:  0.19542272725412924\n",
      "training loss:  0.19622852485026082\n",
      "training loss:  0.17216252751888766\n",
      "training loss:  0.20304293305643142\n",
      "training loss:  0.22269803814349637\n",
      "training loss:  0.21517274072093348\n",
      "training loss:  0.19650771733253233\n",
      "training loss:  0.20636621502137953\n",
      "training loss:  0.21232315705668542\n",
      "training loss:  0.2170046543882739\n",
      "training loss:  0.2143001897681144\n",
      "training loss:  0.20551757177998942\n",
      "training loss:  0.18827713545291655\n",
      "training loss:  0.10829899868657512\n",
      "training loss:  0.1466284870140014\n",
      "training loss:  0.15409159660339355\n",
      "training loss:  0.12704310830562346\n",
      "training loss:  0.11043080014567222\n",
      "training loss:  0.10480928853634865\n",
      "training loss:  0.14434852427051914\n",
      "training loss:  0.1330726747551272\n",
      "training loss:  0.16105605806073836\n",
      "training loss:  0.1202852048220173\n",
      "training loss:  0.1394379384094669\n",
      "training loss:  0.10933708039022261\n",
      "training loss:  0.12908098005479382\n",
      "training loss:  0.13967922281834386\n",
      "training loss:  0.1230266627765471\n",
      "training loss:  0.13537244787139277\n",
      "training loss:  0.13654142954657156\n",
      "training loss:  0.13366521221976127\n",
      "training loss:  0.13234445644963172\n",
      "training loss:  0.14972236656373547\n",
      "training loss:  0.13385896432784297\n",
      "training loss:  0.14318880006190268\n",
      "training loss:  0.12014745800725875\n",
      "training loss:  0.13888037493152003\n",
      "training loss:  0.13826741326239803\n",
      "training loss:  0.15412759636679002\n",
      "training loss:  0.1260036148371235\n",
      "training loss:  0.15434748511160573\n",
      "training loss:  0.14691067655240336\n",
      "training loss:  0.1809878118576542\n",
      "training loss:  0.13402633609310274\n",
      "training loss:  0.16227669100607595\n",
      "training loss:  0.12086096021436876\n",
      "training loss:  0.1443416505090652\n",
      "training loss:  0.1570950150489807\n",
      "training loss:  0.15059788525104523\n",
      "training loss:  0.12806384073149774\n",
      "training loss:  0.13516689164023246\n",
      "training loss:  0.12276880298891375\n",
      "training loss:  0.11378167425432513\n",
      "training loss:  0.14971156562528304\n",
      "training loss:  0.17461336331982766\n",
      "training loss:  0.13804589740691647\n",
      "training loss:  0.14764620219507524\n",
      "training loss:  0.12844125397743716\n",
      "training loss:  0.13039907092048275\n",
      "training loss:  0.13148940570892825\n",
      "training loss:  0.15266714461388126\n",
      "training loss:  0.1555894834379996\n",
      "training loss:  0.18944507645022485\n",
      "training loss:  0.1420276241917764\n",
      "training loss:  0.1512592593508382\n",
      "training loss:  0.14882949667592202\n",
      "training loss:  0.13400997702152498\n",
      "training loss:  0.14464596250364858\n",
      "training loss:  0.11157635911818474\n",
      "training loss:  0.13163734828272172\n",
      "training loss:  0.14789469492050908\n",
      "training loss:  0.1346182962579112\n",
      "training loss:  0.1373862653009353\n",
      "training loss:  0.10854363153057714\n",
      "training loss:  0.08356160838757792\n",
      "training loss:  0.07534673425459093\n",
      "training loss:  0.09308486840417309\n",
      "training loss:  0.11839242039188262\n",
      "training loss:  0.08475183431179292\n",
      "training loss:  0.10514900713197646\n",
      "training loss:  0.09485917129824238\n",
      "training loss:  0.11241865398422364\n",
      "training loss:  0.09651568147443956\n",
      "training loss:  0.07541922455833804\n",
      "training loss:  0.09491169356530713\n",
      "training loss:  0.07694890999024914\n",
      "training loss:  0.10180544709005664\n",
      "training loss:  0.08988935380212722\n",
      "training loss:  0.09660729910096814\n",
      "training loss:  0.09518288172060443\n",
      "training loss:  0.07990109584023876\n",
      "training loss:  0.07939012396720148\n",
      "training loss:  0.09183027955793566\n",
      "training loss:  0.10109591195660253\n",
      "training loss:  0.0971640593582584\n",
      "training loss:  0.11163105359000544\n",
      "training loss:  0.0889488612451861\n",
      "training loss:  0.139007929352022\n",
      "training loss:  0.10015093463082467\n",
      "training loss:  0.11750208610488523\n",
      "training loss:  0.1016170738204833\n",
      "training loss:  0.09067969793273557\n",
      "training loss:  0.09088076122345463\n",
      "training loss:  0.11565384797511562\n",
      "training loss:  0.09766171968752338\n",
      "training loss:  0.09049092617727095\n",
      "training loss:  0.09198840010550714\n",
      "training loss:  0.11848808825016022\n",
      "training loss:  0.10890512216475702\n",
      "training loss:  0.1115654606011606\n",
      "training loss:  0.11807008472181135\n",
      "training loss:  0.10537412810710169\n",
      "training loss:  0.11002949168605189\n",
      "training loss:  0.10181194399633715\n",
      "training loss:  0.08185011532998854\n",
      "training loss:  0.10722880642260274\n",
      "training loss:  0.10373540655259163\n",
      "training loss:  0.10870208471052108\n",
      "training loss:  0.08991271786151393\n",
      "training loss:  0.07002291131404138\n",
      "training loss:  0.072259085793649\n",
      "training loss:  0.09359583018287536\n",
      "training loss:  0.08874607134249902\n",
      "training loss:  0.07935836045972762\n",
      "training loss:  0.10414334602894322\n",
      "training loss:  0.10577260582677779\n",
      "training loss:  0.09718827759065936\n",
      "training loss:  0.10785942740978734\n",
      "training loss:  0.11778713618555377\n",
      "training loss:  0.09372690848765834\n",
      "training loss:  0.1073596429440283\n",
      "training loss:  0.08287979998896199\n",
      "training loss:  0.09713078698804302\n",
      "training loss:  0.05575310414837253\n",
      "training loss:  0.05153342023972542\n",
      "training loss:  0.05567594305161507\n",
      "training loss:  0.05857746783764132\n",
      "training loss:  0.07687931868337816\n",
      "training loss:  0.06464833309573512\n",
      "training loss:  0.07212381161028339\n",
      "training loss:  0.0649152849951098\n",
      "training loss:  0.07147898885511583\n",
      "training loss:  0.05396276424008031\n",
      "training loss:  0.06467367756751276\n",
      "training loss:  0.06981063465918264\n",
      "training loss:  0.05351276791864826\n",
      "training loss:  0.08571726708642897\n",
      "training loss:  0.06319493247616675\n",
      "training loss:  0.05113733680017533\n",
      "training loss:  0.06696484117738662\n",
      "training loss:  0.06147493229758355\n",
      "training loss:  0.06699740742483447\n",
      "training loss:  0.080045941375917\n",
      "training loss:  0.07839799648331057\n",
      "training loss:  0.08146745927872197\n",
      "training loss:  0.08405318760102795\n",
      "training loss:  0.06627162954499645\n",
      "training loss:  0.07174701844492266\n",
      "training loss:  0.0828257534773119\n",
      "training loss:  0.07406730901810431\n",
      "training loss:  0.08630144451895068\n",
      "training loss:  0.06881787123218659\n",
      "training loss:  0.07013117738308446\n",
      "training loss:  0.08037434999019868\n",
      "training loss:  0.06550210762408472\n",
      "training loss:  0.0751128783149104\n",
      "training loss:  0.08280014703350683\n",
      "training loss:  0.06897762658134583\n",
      "training loss:  0.06248993210254177\n",
      "training loss:  0.05698123574256897\n",
      "training loss:  0.06618849596669597\n",
      "training loss:  0.06703968634528498\n",
      "training loss:  0.055311949983719855\n",
      "training loss:  0.07200631451222204\n",
      "training loss:  0.07106449190647371\n",
      "training loss:  0.07173833443272498\n",
      "training loss:  0.07366607890975091\n",
      "training loss:  0.07351936832551033\n",
      "training loss:  0.07536290345653411\n",
      "training loss:  0.07126052389221807\n",
      "training loss:  0.06785734334299641\n",
      "training loss:  0.07831614104009443\n",
      "training loss:  0.0978930885753324\n",
      "training loss:  0.06475267198777968\n",
      "training loss:  0.05906121961532101\n",
      "training loss:  0.07987057345528756\n",
      "training loss:  0.07593239555435796\n",
      "training loss:  0.08909316409018732\n",
      "training loss:  0.06883466387948682\n",
      "training loss:  0.08984392208437766\n",
      "training loss:  0.0860501229763031\n",
      "training loss:  0.06847296991655903\n",
      "training loss:  0.0690703577091617\n",
      "training loss:  0.04412561895385865\n",
      "training loss:  0.03662125333662956\n",
      "training loss:  0.05258879065513611\n",
      "training loss:  0.06158196685775634\n",
      "training loss:  0.05211767217805309\n",
      "training loss:  0.03599500992605763\n",
      "training loss:  0.05699772363708865\n",
      "training loss:  0.04627752448281934\n",
      "training loss:  0.054197909851228035\n",
      "training loss:  0.053668410547317996\n",
      "training loss:  0.049467645825878266\n",
      "training loss:  0.06959241388305541\n",
      "training loss:  0.04486319663063172\n",
      "training loss:  0.052736433763657845\n",
      "training loss:  0.042770509758303245\n",
      "training loss:  0.04209616732212805\n",
      "training loss:  0.051454423416045406\n",
      "training loss:  0.040353466906855186\n",
      "training loss:  0.06320673804129323\n",
      "training loss:  0.048865039502420736\n",
      "training loss:  0.06366136285566515\n",
      "training loss:  0.05557524052358443\n",
      "training loss:  0.043279393065360286\n",
      "training loss:  0.0511391427247755\n",
      "training loss:  0.05551113909290683\n",
      "training loss:  0.060811651810523004\n",
      "training loss:  0.05598491718692164\n",
      "training loss:  0.060127724082239216\n",
      "training loss:  0.06496726457149751\n",
      "training loss:  0.04734590168922178\n",
      "training loss:  0.05166128758461245\n",
      "training loss:  0.05297721778192828\n",
      "training loss:  0.047115184126361724\n",
      "training loss:  0.05680495067950218\n",
      "training loss:  0.04846108248156886\n",
      "training loss:  0.051164078135644234\n",
      "training loss:  0.06476471597148527\n",
      "training loss:  0.06109857895681935\n",
      "training loss:  0.054619218072583596\n",
      "training loss:  0.06513426573045793\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss:  0.04794086419766949\n",
      "training loss:  0.06587965257706181\n",
      "training loss:  0.06903669190022253\n",
      "training loss:  0.06157579393156113\n",
      "training loss:  0.0475217483697399\n",
      "training loss:  0.04704672867251981\n",
      "training loss:  0.06818492470249053\n",
      "training loss:  0.05988139779336991\n",
      "training loss:  0.07875319305927522\n",
      "training loss:  0.06552918255329132\n",
      "training loss:  0.04627080934662973\n",
      "training loss:  0.06531829507120195\n",
      "training loss:  0.06739736228219924\n",
      "training loss:  0.05014564914088095\n",
      "training loss:  0.06203050671085235\n",
      "training loss:  0.05869610319214483\n",
      "training loss:  0.06709461346749336\n",
      "training loss:  0.055022682393750834\n",
      "training loss:  0.05848267386036535\n",
      "training loss:  0.0713609616842962\n",
      "training loss:  0.03204964630065426\n",
      "training loss:  0.04101935990395084\n",
      "training loss:  0.05061068169532284\n",
      "training loss:  0.03958578840378792\n",
      "training loss:  0.03831306003755139\n",
      "training loss:  0.04301358326788871\n",
      "training loss:  0.04886815336442763\n",
      "training loss:  0.04326086178902657\n",
      "training loss:  0.04719824319885623\n",
      "training loss:  0.049374304471477386\n",
      "training loss:  0.05234970536924178\n",
      "training loss:  0.028504610061645508\n",
      "training loss:  0.04869725915693467\n",
      "training loss:  0.04925032296488362\n",
      "training loss:  0.0418270307202493\n",
      "training loss:  0.03837511904778019\n",
      "training loss:  0.04342682082806864\n",
      "training loss:  0.06108030676841736\n",
      "training loss:  0.03817596983525061\n",
      "training loss:  0.046237419689855265\n",
      "training loss:  0.04713251417683017\n",
      "training loss:  0.034275128956763976\n",
      "training loss:  0.03620125978223739\n",
      "training loss:  0.0367909780433101\n",
      "training loss:  0.051086633436141476\n",
      "training loss:  0.04841527583137635\n",
      "training loss:  0.05555359298183072\n",
      "training loss:  0.05812914429172393\n",
      "training loss:  0.050600910379040624\n",
      "training loss:  0.0383469577758543\n",
      "training loss:  0.048864204076028636\n",
      "training loss:  0.038390416772134846\n",
      "training loss:  0.039331264072848905\n",
      "training loss:  0.04208893545212284\n",
      "training loss:  0.04405833059741605\n",
      "training loss:  0.03357021366396258\n",
      "training loss:  0.042279230010124944\n",
      "training loss:  0.03976969949660763\n",
      "training loss:  0.03424739933783008\n",
      "training loss:  0.040222712101474885\n",
      "training loss:  0.06248531514598477\n",
      "training loss:  0.04463876158960404\n",
      "training loss:  0.05633964750074571\n",
      "training loss:  0.044285143575360696\n",
      "training loss:  0.05401468901864944\n",
      "training loss:  0.03852039912054615\n",
      "training loss:  0.051767141588272586\n",
      "training loss:  0.04410851866968216\n",
      "training loss:  0.056585118655235536\n",
      "training loss:  0.0495802269827935\n",
      "training loss:  0.0392791667292195\n",
      "training loss:  0.0479489141894925\n",
      "training loss:  0.04671150542074634\n",
      "training loss:  0.044776320457458496\n",
      "training loss:  0.048030139938477545\n",
      "training loss:  0.05190834883720644\n",
      "training loss:  0.03941594160372211\n",
      "training loss:  0.03650705324065301\n",
      "training loss:  0.042256837890994166\n",
      "training loss:  0.043334468237815366\n",
      "training loss:  0.02862066222775367\n",
      "training loss:  0.03844306882350675\n",
      "training loss:  0.02634512128368501\n",
      "training loss:  0.030617504350600705\n",
      "training loss:  0.03285131675581778\n",
      "training loss:  0.019116371870040894\n",
      "training loss:  0.03667544213033492\n",
      "training loss:  0.03158806600878315\n",
      "training loss:  0.025547079501613494\n",
      "training loss:  0.03069280424425679\n",
      "training loss:  0.029727592583625548\n",
      "training loss:  0.03209984302520752\n",
      "training loss:  0.03058550146318251\n",
      "training loss:  0.027600839253394835\n",
      "training loss:  0.038970193555278164\n",
      "training loss:  0.030777657224285986\n",
      "training loss:  0.02995944071200586\n",
      "training loss:  0.040091116101511066\n",
      "training loss:  0.03169207611391621\n",
      "training loss:  0.030483650584374706\n",
      "training loss:  0.03649224677393513\n",
      "training loss:  0.0325239118068449\n",
      "training loss:  0.029221525596034144\n",
      "training loss:  0.03747341902025284\n",
      "training loss:  0.03225397534908787\n",
      "training loss:  0.03378754661929223\n",
      "training loss:  0.027930533693682764\n",
      "training loss:  0.026904944931307146\n",
      "training loss:  0.03204815522316964\n",
      "training loss:  0.023301239936582504\n",
      "training loss:  0.030112310763328307\n",
      "training loss:  0.04807961179364112\n",
      "training loss:  0.025995753465160247\n",
      "training loss:  0.028973259272113923\n",
      "training loss:  0.023735375173630252\n",
      "training loss:  0.03128055314863882\n",
      "training loss:  0.03525857098640934\n",
      "training loss:  0.031256339242381435\n",
      "training loss:  0.03185451030731201\n",
      "training loss:  0.025955600123251637\n",
      "training loss:  0.05751350810450892\n",
      "training loss:  0.04212018991670301\n",
      "training loss:  0.03186108508417683\n",
      "training loss:  0.03356652394417794\n",
      "training loss:  0.04424950384324597\n",
      "training loss:  0.03270932359080161\n",
      "training loss:  0.03342507443120403\n",
      "training loss:  0.028538211699455016\n",
      "training loss:  0.030278414968521364\n",
      "training loss:  0.045062027631267425\n",
      "training loss:  0.03246733546257019\n",
      "training loss:  0.03888961288236802\n",
      "training loss:  0.0445256031328632\n",
      "training loss:  0.040965407606094115\n",
      "training loss:  0.044689936503287286\n",
      "training loss:  0.02889251516711327\n",
      "training loss:  0.041390099833088535\n",
      "training loss:  0.0357448564421746\n",
      "training loss:  0.03839100560834331\n",
      "training loss:  0.03708865289245882\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.001\n",
    "batch_size = 32\n",
    "epochs = 10\n",
    "logstep = int(1000 // batch_size)\n",
    "\n",
    "train_loader = datatorch.DataLoader(dataset=train_dataset, \n",
    "                         shuffle=True, \n",
    "                         batch_size=batch_size)\n",
    "\n",
    "#test_loader = datatorch.DataLoader(dataset=test_dataset, shuffle = False, batch_size= batch_size)\n",
    "\n",
    "val_loader = datatorch.DataLoader(dataset=test_dataset, shuffle = False, batch_size= 1)\n",
    "\n",
    "#model.fc = nn.Sequential(nn.Linear(model.fc.in_features,512),\n",
    "                                  #nn.ReLU(),\n",
    "                                  #nn.Dropout(),\n",
    "                                  #nn.Linear(512, 2))\n",
    "            \n",
    "model.fc = nn.Sequential(nn.Linear(model.fc.in_features,512),\n",
    "                                  #nn.ReLU(),\n",
    "                                  \n",
    "                                  nn.Linear(512, 4096))\n",
    "\n",
    "#net = TripletNet(resnet101())\n",
    "           \n",
    "            \n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "#device = torch.device(\"cuda:0\")\n",
    "model =model.to(device)\n",
    "#net = torch.nn.DataParallel(net).cuda()\n",
    "#cudnn.benchmark = True\n",
    " #create optimizer\n",
    "criterion = nn.TripletMarginLoss(margin=1.0, p=2)\n",
    "optimizer = torch.optim.SGD(model.parameters(),lr=learning_rate,momentum=0.9,weight_decay=1e-5,nesterov=True)\n",
    "\n",
    "#scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer,mode='min',factor=0.1,patience=10,verbose=True)\n",
    "#optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "#scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[500,1000,1500], gamma=0.5)\n",
    "\n",
    "training_loss_vec = []\n",
    "training_accuracy_vec = []\n",
    "val_f1_score = []\n",
    "    \n",
    "#criterion = torch.nn.CrossEntropyLoss()\n",
    "#optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "#optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "#scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[500,1000,1500], gamma=0.5)\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "# loop over epochs\n",
    "model.train()\n",
    "for e in range(epochs):\n",
    "    training_loss = 0.\n",
    "    training_accuracy = 0.\n",
    "    for idx, (data1, data2, data3) in enumerate(train_loader):\n",
    "    #for idx, (img,label) in enumerate(train_loader):\n",
    "        data1, data2, data3 = data1.cuda(), data2.cuda(), data3.cuda()\n",
    "        #img, label = img.cuda(), label.cuda()\n",
    "        #embedded_a, embedded_p, embedded_n = net(data1, data2, data3)\n",
    "        embedded_a, embedded_p, embedded_n = model(data1), model(data2), model(data3)\n",
    "        loss = criterion(embedded_a, embedded_p, embedded_n)\n",
    "        # call optimizer.zero_grad()\n",
    "        optimizer.zero_grad()\n",
    "        # compute predictions using model\n",
    "        #y_pred =  model(img)\n",
    "        # compute loss\n",
    "        \n",
    "        #loss = criterion(y_pred,label)\n",
    "        # run backward method\n",
    "        loss.backward()\n",
    "        # run optimizer step\n",
    "        optimizer.step()\n",
    "        #scheduler.step()  ######\n",
    "        # logging (optional)\n",
    "        \n",
    "        training_loss += loss.item()\n",
    "        #y_pred_idx = torch.max(y_pred.detach().cpu(),dim=1)[1]\n",
    "        #training_accuracy += torch.mean((y_pred_idx == label.cpu()).float()).item()\n",
    "        if (idx+1) % logstep == 0: \n",
    "            training_loss_vec.append(training_loss/logstep)\n",
    "            #training_accuracy_vec.append(training_accuracy/logstep)\n",
    "            print('training loss: ', training_loss/logstep)\n",
    "            training_loss, training_accuracy = 0.,0.\n",
    "   \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_triplets_pred = []\n",
    "model.eval()\n",
    "for idx, (data1, data2, data3) in enumerate(val_loader):\n",
    "    data1, data2, data3 = data1.cuda(), data2.cuda(), data3.cuda()\n",
    "    embedded_1, embedded_2, embedded_3 = model(data1), model(data2), model(data3)\n",
    "    if torch.dist(embedded_1,embedded_3,2)>=torch.dist(embedded_1,embedded_2,2):\n",
    "        val_triplets_pred.append(str(1))\n",
    "    else:\n",
    "        val_triplets_pred.append(str(0))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = datatorch.DataLoader(dataset=test_dataset, shuffle = False, batch_size= 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 6, got 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-898a4d6dd3d3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mtest_triplets_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mdata1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname3\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_loader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[0mdata1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata3\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata3\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0membedded_1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0membedded_2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0membedded_3\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: not enough values to unpack (expected 6, got 3)"
     ]
    }
   ],
   "source": [
    "test_triplets_pred = []\n",
    "model.eval()\n",
    "for idx, data1, data2, data3 in enumerate(test_loader):\n",
    "    data1, data2, data3 = data1.cuda(), data2.cuda(), data3.cuda()\n",
    "    embedded_1, embedded_2, embedded_3 = model(data1), model(data2), model(data3)\n",
    "    if torch.dist(embedded_1,embedded_3,2)>=torch.dist(embedded_1,embedded_2,2):\n",
    "        test_triplets_pred.append(str(1))\n",
    "    else:\n",
    "        test_triplets_pred.append(str(0))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(test_triplets_pred))\n",
    "print(str(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('submission_Ketzel.txt', 'w') as f:\n",
    "    for item in test_triplets_pred:\n",
    "        f.write(item + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
